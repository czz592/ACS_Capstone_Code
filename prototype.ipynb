{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6db16f2",
   "metadata": {},
   "source": [
    "# Prototyping Notebook\n",
    "\n",
    "This notebook aims to begin prototyping the IDS, implementing the DNN models, FL server and client logic, and incorporate PETs and XAI components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b05f49",
   "metadata": {},
   "source": [
    "## General\n",
    "\n",
    "This section performs data loading, exploring, and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456aebb",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DL\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# PETs\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "\n",
    "\"\"\"\n",
    "# FL\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 32\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b975f7a",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- Load data\n",
    "- Analyse\n",
    "  - Display shape, nulls, and dtypes\n",
    "  - Analyse categorical and numerical splits\n",
    "  - Detect sparsity and constant columns\n",
    "  - Detect correlations and important features\n",
    "  - Identify outliers or skewed distributions\n",
    "  - Anaylse target distributions\n",
    "- Preprocess\n",
    "  - Remove redundant columns\n",
    "  - Remove duplicate values\n",
    "  - Remove high 0 value columns\n",
    "  - Encode categorical columns\n",
    "  - Scale numerical columns\n",
    "  - Transform to tensor\n",
    "  - Wrap with Dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d42a22",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_df(df: pd.DataFrame):\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumn types:\\n\", df.dtypes.value_counts())\n",
    "    print(\"\\nMissing values per column:\\n\",\n",
    "          df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    print(f\"\\nConstant columns:\\n{df.nunique()[df.nunique() <= 1]}\\n\")\n",
    "    df.info(verbose=True, show_counts=True, max_cols=None)\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "def show_target_distribution(df: pd.DataFrame, target_col='Attack_type'):\n",
    "    print(df[target_col].value_counts())\n",
    "    sns.countplot(data=df, y=target_col,\n",
    "                  order=df[target_col].value_counts().index)\n",
    "    plt.title(f\"Distribution of {target_col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if 'Attack_label' in df.columns:\n",
    "        sns.countplot(data=df, x='Attack_label')\n",
    "        plt.title(\"Binary Attack Label Distribution\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(df[target_col].value_counts())\n",
    "        print(df['Attack_label'].value_counts())\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, threshold: float = 0.9):\n",
    "    corr = df.select_dtypes(include='number').corr()\n",
    "    # Identify highly correlated pairs\n",
    "    high_corr = ((corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                  .stack()\n",
    "                  .reset_index()\n",
    "                  .rename(columns={0: 'correlation'}))\n",
    "                 .query('abs(correlation) > @threshold'))\n",
    "    print(\"Highly correlated features (>|0.9|):\\n\", high_corr)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "def visualise_df(df: pd.DataFrame):\n",
    "    summarise_df(df)\n",
    "    show_target_distribution(df)\n",
    "    plot_correlation_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1baa7",
   "metadata": {},
   "source": [
    "#### Data Loading & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7641a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'dataset/edge-iiotset/eval/DNN-EdgeIIoT-dataset.csv'\n",
    "data_path = 'dataset/edge-iiotset/eval/ML-EdgeIIoT-dataset.csv'\n",
    "# data_path = 'dataset\\ciciot2023\\MERGED_CSV\\Merged01.csv'\n",
    "df = pd.read_csv(data_path, encoding='utf-8', low_memory=False)\n",
    "\n",
    "visualise_df(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edcab6",
   "metadata": {},
   "source": [
    "#### Dropping Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1428c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not useful\n",
    "original_drop_columns = [\n",
    "    \"frame.time\", \n",
    "    \"ip.src_host\",\n",
    "    \"ip.dst_host\", \n",
    "    \"arp.src.proto_ipv4\",\n",
    "    \"arp.dst.proto_ipv4\",\n",
    "    \"http.file_data\",\n",
    "    \"http.request.full_uri\",\n",
    "    \"icmp.transmit_timestamp\",\n",
    "    \"http.request.uri.query\",\n",
    "    \"tcp.options\",\n",
    "    \"tcp.payload\",\n",
    "    \"tcp.srcport\",\n",
    "    \"tcp.dstport\",\n",
    "    \"udp.port\",\n",
    "    \"mqtt.msg\"\n",
    "]\n",
    "additional_drop_columns = [\n",
    "    \"icmp.unused\",\n",
    "    \"http.tls_port\",\n",
    "    \"dns.qry.type\"\n",
    "]\n",
    "\n",
    "print(df.shape)\n",
    "print(f\"Dropping {len(original_drop_columns)} columns: \", original_drop_columns)\n",
    "df.drop(original_drop_columns, axis=1, inplace=True)\n",
    "\n",
    "print(f\"Dropping {len(additional_drop_columns)} columns: \",\n",
    "      additional_drop_columns)\n",
    "df.drop(additional_drop_columns, axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "print(df.isna().any(axis=1).sum(), \"rows with at least one NaN to remove\")\n",
    "\n",
    "df = df.dropna(axis=0, how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c870b4e",
   "metadata": {},
   "source": [
    "#### Sparse Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89352f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with high 0 values\n",
    "sparse_columns = []\n",
    "# identify numerical and categorical columns\n",
    "numerical_col = []\n",
    "categorical_col = []\n",
    "\n",
    "def convert_to_float(value):\n",
    "    if value in [0, 0.0, '0', '0.0', None]:\n",
    "        return float(0)\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None  # or handle as needed\n",
    "\n",
    "for col in df.columns:\n",
    "    # apply the conversion function to all columns\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "    if col != 'Attack_label' and col != 'Attack_type':\n",
    "        try:\n",
    "            # calculate sparsity\n",
    "            zero_ratio = df[col].eq(0).sum() / len(df)\n",
    "            if zero_ratio >= 0.90:\n",
    "                sparse_columns.append((col, zero_ratio))\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            # remove constant columns\n",
    "            elif df[col].nunique() == 1:\n",
    "                print(\"dropping constant column: \", col)\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            # classify as numerical or categorical for later use\n",
    "            elif df[col].dtype == object or df[col].nunique() <= 10:\n",
    "                categorical_col.append(col)\n",
    "            else:\n",
    "                numerical_col.append(col)\n",
    "        except:\n",
    "            # skip non-numeric or problematic columns\n",
    "            continue\n",
    "\n",
    "# Display the results\n",
    "print(\"Sparse columns: \", len(sparse_columns))\n",
    "for col, ratio in sparse_columns:\n",
    "    print(f\"{col}: {ratio:.2%} zeros\")\n",
    "\n",
    "print(\"New DF Shape: \" , df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20859dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_col:\n",
    "    print(df[col].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f402926",
   "metadata": {},
   "source": [
    "#### Drop Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.duplicated().sum(), \"fully duplicate rows to remove\")\n",
    "df.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea08f8f",
   "metadata": {},
   "source": [
    "#### Feature Correlation with Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the correlation of all columns with attack_label\n",
    "df.corrwith(df['Attack_label']).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd57e03",
   "metadata": {},
   "source": [
    "#### Remove Category Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Attack_type'], axis=1, inplace=True)\n",
    "\n",
    "features = df.drop('Attack_label', axis=1)\n",
    "labels = df['Attack_label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f6e9a",
   "metadata": {},
   "source": [
    "### Tensorise and Wrap with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21544188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * 0.8)\n",
    "test_size = len(df) - train_size\n",
    "batch_size = 64\n",
    "\n",
    "# tensorise features and labels\n",
    "features = torch.tensor(features.values, dtype=torch.float32)\n",
    "labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "train_features, inputs, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "test_dataset = TensorDataset(inputs, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834615fb",
   "metadata": {},
   "source": [
    "## Phase 1 - Centralised DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd929ae",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfbe1a",
   "metadata": {},
   "source": [
    "#### DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        \"\"\"\n",
    "        Define model architecture here.\\n\n",
    "        \n",
    "        Version 1: 03/07\n",
    "        - Basic Fully Connected Neural Network\n",
    "        \"\"\"\n",
    "        super(DNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4b02c",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Define model architecture here.\n",
    "        \n",
    "        TODO:\n",
    "        - Adaptive layer sizes based in data shape\n",
    "        \n",
    "        Version 2: 07/07\n",
    "        - Refined boilerplate model\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # TODO: calculate correct layer sizes\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # add channel dimension\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc1cf0",
   "metadata": {},
   "source": [
    "### Training and Testing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb2216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model: nn.Module, loss_fn, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(f\"Training model {model.__class__.__name__}\")\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(input_dim=features.shape[1], hidden_dim=128, output_dim=2)\n",
    "dnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(dnn.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):\n",
    "    train(train_loader, dnn, criterion, optimiser)\n",
    "    test(test_loader, dnn, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa776530",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):\n",
    "    train(train_loader, cnn, criterion, optimiser)\n",
    "    test(test_loader, cnn, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf097116",
   "metadata": {},
   "source": [
    "## Phase 2 - PETs\n",
    "\n",
    "This part would be done on all devices training their local model as it is to be integrated into the model training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52576c0e",
   "metadata": {},
   "source": [
    "### Differential Privacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19046619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise multiplier & max grad norm\n",
    "noise_multiplier = 0.2\n",
    "max_grad_norm = 1\n",
    "\n",
    "# DP\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab57409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model, optimiser & loss function\n",
    "model = DNN(input_dim=features.shape[1], hidden_dim=128, output_dim=2)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "dnn_gc, optimiser_gc, criterion_gc, train_loader_gc = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimiser,\n",
    "    criterion=criterion,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    grad_sample_mode=\"ghost\"\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    train(train_loader_gc, dnn_gc, criterion_gc, optimiser_gc)\n",
    "    test(test_loader, dnn_gc, criterion_gc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model, optimiser & loss function\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "cnn_gc, optimiser_gc, criterion_gc, train_loader_gc = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimiser,\n",
    "    criterion=criterion,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    grad_sample_mode=\"ghost\"\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    train(train_loader_gc, cnn_gc, criterion_gc, optimiser_gc)\n",
    "    test(test_loader, cnn_gc, criterion_gc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
