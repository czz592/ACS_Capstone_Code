{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6db16f2",
   "metadata": {},
   "source": [
    "# Prototyping Notebook\n",
    "\n",
    "This notebook aims to begin prototyping the IDS, implementing the DNN models, FL server and client logic, and incorporate PETs and XAI components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b05f49",
   "metadata": {},
   "source": [
    "## General\n",
    "\n",
    "This section performs data loading, exploring, and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456aebb",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DL\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# PETs\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "\n",
    "\"\"\"\n",
    "# FL\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 64\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2092a1",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not useful\n",
    "def custom_drop_cols(df: pd.DataFrame, drop_cols: list):\n",
    "    print(df.shape)\n",
    "    print(f\"Dropping {len(drop_cols)} columns: \", drop_cols)\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231b3b0",
   "metadata": {},
   "source": [
    "#### EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_df(df: pd.DataFrame):\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumn types:\\n\", df.dtypes.value_counts())\n",
    "    print(\"\\nMissing values per column:\\n\",\n",
    "          df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    print(f\"\\nConstant columns:\\n{df.nunique()[df.nunique() <= 1]}\\n\")\n",
    "    df.info(verbose=True, show_counts=True, max_cols=None)\n",
    "    print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_target_distribution(df: pd.DataFrame, target_col='Attack_type'):\n",
    "    print(df[target_col].value_counts())\n",
    "    sns.countplot(data=df, y=target_col,\n",
    "                  order=df[target_col].value_counts().index)\n",
    "    plt.title(f\"Distribution of {target_col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if 'Attack_label' in df.columns:\n",
    "        sns.countplot(data=df, x='Attack_label')\n",
    "        plt.title(\"Binary Attack Label Distribution\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(df[target_col].value_counts())\n",
    "        print(df['Attack_label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df: pd.DataFrame, threshold: float = 0.9):\n",
    "    corr = df.select_dtypes(include='number').corr()\n",
    "    # Identify highly correlated pairs\n",
    "    high_corr = ((corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                  .stack()\n",
    "                  .reset_index()\n",
    "                  .rename(columns={0: 'correlation'}))\n",
    "                 .query('abs(correlation) > @threshold'))\n",
    "    print(\"Highly correlated features (>|0.9|):\\n\", high_corr)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e7756",
   "metadata": {},
   "source": [
    "#### Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model: nn.Module, loss_fn, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(f\"Training model {model.__class__.__name__}\")\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b975f7a",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- Load data\n",
    "- Analyse\n",
    "  - Display shape, nulls, and dtypes\n",
    "  - Analyse categorical and numerical splits\n",
    "  - Detect sparsity and constant columns\n",
    "  - Detect correlations and important features\n",
    "  - Identify outliers or skewed distributions\n",
    "  - Anaylse target distributions\n",
    "- Preprocess\n",
    "  - Remove redundant columns\n",
    "  - Remove duplicate values\n",
    "  - Remove high 0 value columns\n",
    "  - Encode categorical columns\n",
    "  - Scale numerical columns\n",
    "  - Transform to tensor\n",
    "  - Wrap with Dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1baa7",
   "metadata": {},
   "source": [
    "#### Loading & Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7641a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge-iiot\n",
    "# data_path = 'dataset/edge-iiotset/eval/DNN-EdgeIIoT-dataset.csv'\n",
    "data_path = 'dataset/edge-iiotset/eval/ML-EdgeIIoT-dataset.csv'\n",
    "\n",
    "# ciciot\n",
    "# data_path = 'dataset\\ciciot2023\\MERGED_CSV\\Merged01.csv'\n",
    "df = pd.read_csv(data_path, encoding='utf-8', low_memory=False)\n",
    "\n",
    "summarise_df(df)\n",
    "plot_correlation_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edcab6",
   "metadata": {},
   "source": [
    "#### Dropping Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_drop_columns = [\n",
    "    \"frame.time\",\n",
    "    \"ip.src_host\",\n",
    "    \"ip.dst_host\",\n",
    "    \"arp.src.proto_ipv4\",\n",
    "    \"arp.dst.proto_ipv4\",\n",
    "    \"http.file_data\",\n",
    "    \"http.request.full_uri\",\n",
    "    \"icmp.transmit_timestamp\",\n",
    "    \"http.request.uri.query\",\n",
    "    \"tcp.options\",\n",
    "    \"tcp.payload\",\n",
    "    \"tcp.srcport\",\n",
    "    \"tcp.dstport\",\n",
    "    \"udp.port\",\n",
    "    \"mqtt.msg\"\n",
    "]\n",
    "\n",
    "# new version of cols to drop\n",
    "safe_to_drop_cols = [\n",
    "    \"frame.time\",\n",
    "    \"ip.src_host\",\n",
    "    \"ip.dst_host\",\n",
    "    \"arp.src.proto_ipv4\",\n",
    "    \"arp.dst.proto_ipv4\",\n",
    "    \"http.file_data\",\n",
    "    \"http.request.full_uri\",\n",
    "    \"icmp.transmit_timestamp\",\n",
    "    \"tcp.options\",\n",
    "    \"tcp.payload\",\n",
    "    \"mqtt.msg\",\n",
    "    \"icmp.unused\",\n",
    "    \"mqtt.msg_decoded_as\",\n",
    "    \"Attack_type\"\n",
    "]\n",
    "\n",
    "df = custom_drop_cols(df, safe_to_drop_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c870b4e",
   "metadata": {},
   "source": [
    "#### Sparse Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89352f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with high 0 values\n",
    "sparse_columns = []\n",
    "# identify numerical and categorical columns\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "\n",
    "def convert_to_float(value):\n",
    "    if value in [0, 0.0, '0', '0.0']:\n",
    "        return float(0)\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return value  # or handle as needed\n",
    "\n",
    "for col in df.columns:\n",
    "    # apply the conversion function to all columns\n",
    "    df[col] = df[col].apply(convert_to_float)\n",
    "    if col != 'Attack_label':\n",
    "        try:\n",
    "            # calculate sparsity\n",
    "            zero_ratio = df[col].eq(0).sum() / len(df)\n",
    "            if zero_ratio >= 0.95:\n",
    "                sparse_columns.append((col, zero_ratio))\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            elif df[col].nunique() == 1:\n",
    "                print(\"dropping constant column: \", col)\n",
    "                # df.drop(col, axis=1, inplace=True)\n",
    "            # classify as numerical or categorical for later use\n",
    "            elif df[col].dtype == object or df[col].nunique() <= 10:\n",
    "                categorical_features.append(col)\n",
    "            # else:\n",
    "            #     numerical_features.append(col)\n",
    "        except:\n",
    "            # skip non-numeric or problematic columns\n",
    "            continue\n",
    "\n",
    "# Display the results\n",
    "print(\"Sparse columns: \", len(sparse_columns))\n",
    "for col, ratio in sparse_columns:\n",
    "    print(f\"{col}: {ratio:.2%} zeros\")\n",
    "\n",
    "print(\"New DF Shape: \" , df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20859dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical features\\tNumber of unique values\")\n",
    "for col in categorical_features:\n",
    "    print(col, \"\\t\", df[col].nunique())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd57e03",
   "metadata": {},
   "source": [
    "#### Remove Category Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Attack_label', axis=1)\n",
    "labels = df['Attack_label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3d5d3",
   "metadata": {},
   "source": [
    "### Split into Training and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18cb72",
   "metadata": {},
   "source": [
    "### Encode Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import encoder\n",
    "import category_encoders as ce\n",
    "\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "\n",
    "# low_cardinality_features = ['tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.syn','tcp.flags', 'tcp.flags.ack']\n",
    "low_cardinality_features = [col for col in categorical_features if X_train[col].nunique() <= 10]\n",
    "print(\"Low cardinality features:\\n\",low_cardinality_features)\n",
    "\n",
    "encoder = ce.OneHotEncoder()\n",
    "train_encoded = encoder.fit_transform(X_train[low_cardinality_features])\n",
    "test_encoded = encoder.transform(X_test[low_cardinality_features])\n",
    "\n",
    "X_train.drop(low_cardinality_features, axis=1, inplace=True)\n",
    "X_test.drop(low_cardinality_features, axis=1, inplace=True)\n",
    "\n",
    "X_train = pd.concat([X_train, train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test, test_encoded], axis=1)\n",
    "\n",
    "encoder = ce.CountEncoder()\n",
    "X_train['tcp.srcport'] = encoder.fit_transform(X_train['tcp.srcport'])\n",
    "X_test['tcp.srcport'] = encoder.transform(X_test['tcp.srcport'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine new data\n",
    "print(\"New DF Shape: \" , X_train.shape)\n",
    "print(\"New DF Shape: \" , X_test.shape)\n",
    "\n",
    "print(\"Encoded dataset:\")\n",
    "print(X_train.columns)\n",
    "print(\"Data types of X_train:\\n\", X_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0258337",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [col for col in X_train.columns if X_train[col].dtype in ['float64', 'int64']]\n",
    "print(\"Numerical features:\\n\", numerical_features)\n",
    "\n",
    "assert len(numerical_features) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63cff7",
   "metadata": {},
   "source": [
    "### Scale Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import minmax scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=numerical_features)\n",
    "# X_test = pd.DataFrame(scaler.transform(X_test), columns=numerical_features)\n",
    "\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f6e9a",
   "metadata": {},
   "source": [
    "### Tensorise and Wrap with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21544188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * 0.8)\n",
    "test_size = len(df) - train_size\n",
    "batch_size = 64\n",
    "\n",
    "print(f\"Data types: {X_train.dtypes}\")\n",
    "\n",
    "# tensorise data\n",
    "train_features = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "test_features = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834615fb",
   "metadata": {},
   "source": [
    "## Phase 1 - Centralised DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd929ae",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfbe1a",
   "metadata": {},
   "source": [
    "#### DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        \"\"\"\n",
    "        Define model architecture here.\\n\n",
    "        \n",
    "        Version 1: 03/07\n",
    "        - Basic Fully Connected Neural Network\n",
    "        \"\"\"\n",
    "        super(DNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4b02c",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Define model architecture here.\n",
    "        \n",
    "        TODO:\n",
    "        - Adaptive layer sizes based in data shape\n",
    "        \n",
    "        Version 2: 07/07\n",
    "        - Refined boilerplate model\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # TODO: calculate correct layer sizes\n",
    "            nn.LazyLinear(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # add channel dimension\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(input_dim=X_train.shape[1], hidden_dim=128, output_dim=2)\n",
    "dnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(dnn.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):\n",
    "    train(train_loader, dnn, criterion, optimiser)\n",
    "    test(test_loader, dnn, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa776530",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):\n",
    "    train(train_loader, cnn, criterion, optimiser)\n",
    "    test(test_loader, cnn, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f9ef4d",
   "metadata": {},
   "source": [
    "### XAI for Default Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc296c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the xai stuff for dnn\n",
    "\n",
    "dnn.eval()\n",
    "\n",
    "background = train_features[:100]\n",
    "background = background.to(device)\n",
    "\n",
    "explainer = shap.GradientExplainer(dnn, background)\n",
    "\n",
    "test_data = train_features[100:200]\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "shap_values = explainer.shap_values(test_data)\n",
    "\n",
    "test_data = test_data.cpu().numpy()\n",
    "\n",
    "shap.summary_plot(shap_values, test_data, class_names=[\"Benign\", \"Malicious\"], feature_names=X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature correlation:\")\n",
    "print(X_train.corr())\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(X_train.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf097116",
   "metadata": {},
   "source": [
    "## Phase 2 - PETs\n",
    "\n",
    "This part would be done on all devices training their local model as it is to be integrated into the model training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52576c0e",
   "metadata": {},
   "source": [
    "### Differential Privacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19046619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise multiplier & max grad norm\n",
    "noise_multiplier = 0.2\n",
    "max_grad_norm = 1\n",
    "\n",
    "# DP\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab57409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model, optimiser & loss function\n",
    "model = DNN(input_dim=X_train.shape[1], hidden_dim=128, output_dim=2)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "dnn_gc, optimiser_gc, criterion_gc, train_loader_gc = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimiser,\n",
    "    criterion=criterion,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    grad_sample_mode=\"ghost\"\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    train(train_loader_gc, dnn_gc, criterion_gc, optimiser_gc)\n",
    "    test(test_loader, dnn_gc, criterion_gc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model, optimiser & loss function\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "cnn_gc, optimiser_gc, criterion_gc, train_loader_gc = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimiser,\n",
    "    criterion=criterion,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    grad_sample_mode=\"ghost\"\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    train(train_loader_gc, cnn_gc, criterion_gc, optimiser_gc)\n",
    "    test(test_loader, cnn_gc, criterion_gc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60f591",
   "metadata": {},
   "source": [
    "## Phase 3 - XAI\n",
    "\n",
    "This phase would be completed on the central server to evaluate the explainability of the global model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f500e",
   "metadata": {},
   "source": [
    "### Create Non-DP Model Copy\n",
    "\n",
    "Opacus DP may be incompatible with SHAP explainers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_copy(new_model, old_model):\n",
    "    new_model.load_state_dict(old_model._module.state_dict())\n",
    "    new_model.eval()\n",
    "    new_model.to(device)\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955598da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model copy using dp weights\n",
    "new_model = make_model_copy(DNN(X_train.shape[1], hidden_dim=128, output_dim=2), dnn_gc)\n",
    "# new_model = make_model_copy(CNN(), cnn_gc)\n",
    "\n",
    "# create background dataset\n",
    "background = train_features[:100]\n",
    "background = background.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37768dc6",
   "metadata": {},
   "source": [
    "### SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SHAP explainer\n",
    "explainer = shap.GradientExplainer(new_model,background)\n",
    "\n",
    "# calculate shap_values with test_features\n",
    "shap_values = explainer.shap_values(test_features[:100].to(device))\n",
    "\n",
    "# calculate shap summary plot\n",
    "shap.summary_plot(shap_values, test_features[:100].to(device), class_names=[\"Benign\", \"Malicious\"], feature_names=X_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7dd75",
   "metadata": {},
   "source": [
    "### Captum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c440a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use IntegratedGradients to analyse model\n",
    "ig = IntegratedGradients(new_model)\n",
    "\n",
    "# compute ig values\n",
    "attr, delta = ig.attribute(test_features[:100].to(device), baselines=background, target=0, return_convergence_delta=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af213492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to print importances and visualize distribution\n",
    "def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f' % (importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(x_pos, importances, align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "visualize_importances(features.columns, np.mean(attr, axis=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
